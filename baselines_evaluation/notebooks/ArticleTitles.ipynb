{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from users.users import Users\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_path = ''\n",
    "test_path = ''\n",
    "train_path = ''\n",
    "root_path = ''\n",
    "test_authors_path = ''\n",
    "path_out = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(crawled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_path)\n",
    "df_test_author_ids = df_test['author_id'].unique()\n",
    "# UNCOMMENT\n",
    "# df_train = pd.read_csv('/mnt/data/vikuen/data/guardian/train-set_all.csv')\n",
    "# if for smaller vector\n",
    "df_train = pd.read_csv(train_path)\n",
    "author_ids = df_train[df_train['author_id'].isin(df_test_author_ids)]['author_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_article_ids = df_train[df_train['author_id'].isin(df_test_author_ids)]['article_id'].unique()\n",
    "test_article_ids = df_test['article_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles_train = df[df['id'].isin(train_article_ids)]['title'].tolist()\n",
    "article_titles_train_ids = df[df['id'].isin(train_article_ids)]['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles_test = df[df['id'].isin(test_article_ids)]['title'].tolist()\n",
    "article_titles_test_ids = df[df['id'].isin(test_article_ids)]['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "tfidf_vectorizer = tfidf_vectorizer.fit(article_titles_train)\n",
    "tfidf_train_vectors = tfidf_vectorizer.transform(article_titles_train)\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(article_titles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_comment_to_article = df_test[['article_id', 'comment_id']]\n",
    "df_test_comment_to_article.index = df_test_comment_to_article.comment_id\n",
    "test_comment_to_article_dict = df_test_comment_to_article['article_id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_representation = df_train[df_train['author_id'].isin(df_test_author_ids)]\n",
    "df_train_user_articles = df_representation.groupby('author_id')['article_id'].apply(lambda x: np.unique(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = Users()\n",
    "\n",
    "author_ids = np.load(test_authors_path)\n",
    "print('All Authors: ', len(author_ids))\n",
    "print('Authors with negative samples: ', len(author_ids))\n",
    "\n",
    "# author_ids = np.array_split(author_ids, partitions)\n",
    "# print(f'Number of parititions: {len(author_ids)}')\n",
    "# author_ids = author_ids[n_partition]\n",
    "\n",
    "MAX_POSITIVE_SAMPLES_USER = 10\n",
    "NUM_NEGATIVE_PER_POSITIVE = 50\n",
    "\n",
    "with open(path_out, mode='w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['author_id', 'k', 'hits_at_k', 'interacted_count', 'precision', 'recall', 'AP' 'documents'])\n",
    "\n",
    "K = [1, 3, 5, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_article_pos_dict = {k: v for v, k in enumerate(article_titles_train_ids)} \n",
    "test_article_pos_dict = {k: v for v, k in enumerate(article_titles_test_ids)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "def get_user_representation(user_id):\n",
    "    user_train_articles = df_train_user_articles[df_train_user_articles['author_id'] == user_id]['article_id'].iloc[0]\n",
    "    user_train_mask = []\n",
    "    for article in user_train_articles:\n",
    "        pos = train_article_pos_dict.get(article)\n",
    "        if pos is not None:\n",
    "            user_train_mask.append(pos)\n",
    "    \n",
    "    mx = tfidf_train_vectors[user_train_mask]\n",
    "    return mx.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_section_representation(comment_section_ids):\n",
    "    if len(comment_section_ids) > 0:\n",
    "        article_id = test_comment_to_article_dict[comment_section_ids[0]]\n",
    "        return tfidf_test_vectors[test_article_pos_dict[article_id]].todense()\n",
    "    else: \n",
    "        return np.zeros((1, 26030))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(interacted_items_count_testset, hits, k):\n",
    "    precision = hits / k\n",
    "    recall = hits / interacted_items_count_testset\n",
    "    return interacted_items_count_testset, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(index):\n",
    "    if index % 200 == 0:\n",
    "        print(index / len(author_ids))\n",
    "\n",
    "for index, author in enumerate(author_ids):\n",
    "    print_progress(index)\n",
    "    author_representation = get_user_representation(author)\n",
    "    truth_sections = users.get_positive_test_samples(author)[:MAX_POSITIVE_SAMPLES_USER]\n",
    "    false_sections = users.get_negative_test_samples(author, len(truth_sections), NUM_NEGATIVE_PER_POSITIVE)\n",
    "\n",
    "    representation = np.array([get_comment_section_representation(l) for l in truth_sections])\n",
    "    nsamples, nx, ny = representation.shape\n",
    "    representation = representation.reshape((nsamples,nx*ny))\n",
    "    truth_sim = list(cosine_similarity(author_representation.reshape(1, -1), representation)[0])\n",
    "    \n",
    "    representation = np.array([get_comment_section_representation(l) for l in false_sections])\n",
    "    nsamples, nx, ny = representation.shape\n",
    "    representation = representation.reshape((nsamples,nx*ny))\n",
    "    false_sim = list(cosine_similarity(author_representation.reshape(1, -1), representation)[0])\n",
    "\n",
    "    interacted_items_count_testset = len(truth_sections)\n",
    "\n",
    "    t = pd.DataFrame({'v': truth_sim, 'flag': 1})\n",
    "    f = pd.DataFrame({'v': false_sim, 'flag': 0})\n",
    "    out = pd.concat([t, f])\n",
    "    out = out.sort_values(by='v', ascending=False).head(max(K))\n",
    "    y_true = out['flag'].tolist()\n",
    "    for k in K:\n",
    "        hits_at_k = sum(y_true[:k])\n",
    "        interacted_items_count_testset, precision, recall = evaluate(interacted_items_count_testset, hits_at_k, k)\n",
    "        precisions_at = []\n",
    "        for i, el in enumerate(y_true[:k]):\n",
    "            precisions_at.append(sum(y_true[:i+1]) / (i+1))\n",
    "        AP_at_k = sum(precisions_at) / k\n",
    "        with open(path_out, 'a', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # ['author_id', 'k', 'hits_at_k', 'interacted_count', 'precision', 'recall']\n",
    "            writer.writerow([author, k, hits_at_k, interacted_items_count_testset, precision, recall, round(AP_at_k, 4), len(out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
