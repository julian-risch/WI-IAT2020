{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'CF': '/mnt/data/vikuen/data/daily-mail/results/evaluation_collaborative.csv',\n",
    "    'CF22': '/mnt/data/vikuen/data/daily-mail/results/evaluation_collaborative_22.csv',\n",
    "    'TF-IDF': '/mnt/data/vikuen/data/daily-mail/results/evaluation_bow.csv',\n",
    "    'BRF': '/mnt/data/vikuen/data/daily-mail/results/evaluation_data_fusion.csv',\n",
    "    'node2vec': '/mnt/data/vikuen/data/daily-mail/results/evaluation_data_node2vec_update.csv',\n",
    "    'DeepCoNN': '/mnt/data/vikuen/data/daily-mail/results/evaluation_data_deepconn_fm.csv',    \n",
    "    'HyCoNN': '/mnt/data/vikuen/data/daily-mail/results/evaluation_data_main_model_update.csv',\n",
    "    'NDRF': '/mnt/data/vikuen/data/daily-mail/results/evaluation_data_fusion_node2vec_deepconn.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 3, 5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df):\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1 = []\n",
    "    MAP = []\n",
    "    for k in K:\n",
    "        selected = df[df.k == k] \n",
    "        global_recall_at_k = selected.hits_at_k.sum() / float(selected.interacted_count.sum())\n",
    "        recall = selected.recall.mean()\n",
    "        recalls.append(recall)\n",
    "        precision = selected.precision.mean()\n",
    "        precisions.append(precision)\n",
    "        f1.append((2*(recall * precision)) / (recall + precision))\n",
    "        MAP.append(selected.AP.mean())\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': {},\n",
    "        'f1': {},\n",
    "        'MAP': {},\n",
    "        'recall': {}\n",
    "    }\n",
    "    for i in range(len(K)):\n",
    "        metrics['recall'][f'recall@{K[i]}'] = recalls[i]\n",
    "        metrics['precision'][f'precision@{K[i]}'] = precisions[i]    \n",
    "        metrics['f1'][f'f1@{K[i]}'] = f1[i]    \n",
    "        metrics['MAP'][f'MAP@{K[i]}'] = MAP[i]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['recall', 'precision', 'f1', 'MAP']\n",
    "col_names = ['author_id', 'k', 'hits_at_k', 'interacted_count', 'precision', 'recall', 'AP', 'documents']\n",
    "eval_results = []\n",
    "df = None\n",
    "dfs = []\n",
    "for model, path in models.items():\n",
    "    print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    eval_results.append(evaluate_model(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dfs = []\n",
    "for metric in metrics:\n",
    "    out_dict = {}\n",
    "    out_dict['metric'] = []\n",
    "    for model in models:\n",
    "        out_dict[model] = []\n",
    "    \n",
    "    for k in K:\n",
    "        out_dict['metric'].append(f'{metric}@{k}')\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        results = eval_results[i][metric]\n",
    "        for _, value in results.items():\n",
    "            out_dict[model].append(value)\n",
    "            \n",
    "    out_df = pd.DataFrame(out_dict)\n",
    "    out_df = out_df.round(3)\n",
    "    display(HTML(out_df.to_html()))\n",
    "    print(out_df.to_latex())\n",
    "    out_df.to_csv(f'{metric}.csv', index=False)\n",
    "    out_dfs.append(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recall = out_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recall['recall'] = df_recall['metric'].apply(lambda x: int(x.replace('recall@', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recall = df_recall.drop(columns=['metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recall.index = df_recall.recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df_recall.transpose().stack().reset_index()\n",
    "to_plot = to_plot[to_plot['level_0'] != 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['k'] = to_plot['recall']\n",
    "to_plot['Recall'] = to_plot[0]\n",
    "to_plot = to_plot[to_plot['level_0'].isin( ['CF','node2vec', 'BRF', 'HyCoNN', 'DeepCoNN', 'NDRF', 'TF-IDF'])]\n",
    "# ['CF','node2vec', 'BRF', 'HyCoNN', 'DeepCoNN', 'NDRF', 'TF-IDF']\n",
    "to_plot['Model'] = to_plot['level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context('talk', font_scale=1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9,6)) \n",
    "filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
    "dash_styles = [\"\",\n",
    "               (4, 1.5),\n",
    "               (1, 1),\n",
    "               (3, 1, 1.5, 1),\n",
    "               (5, 1, 1, 1),\n",
    "               (5, 1, 2, 1, 2, 1),\n",
    "               (2, 2, 3, 1.5),\n",
    "               (1, 2.5, 3, 1.2)]\n",
    "ax = sns.lineplot(x='k', y='Recall', hue='Model',style=\"Model\", data=to_plot[to_plot['level_0'] != 'recall'],\n",
    "                  ax=ax, linewidth=3, dashes=dash_styles, markersize=8)\n",
    "plt.setp(ax,xticks=K) \n",
    "leg = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., frameon=False)\n",
    "\n",
    "# set the linewidth of each legend object\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3.0)\n",
    "save_to_pdf('./plots/results_daily-mail.pdf', ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pdf(filename, ax):\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
