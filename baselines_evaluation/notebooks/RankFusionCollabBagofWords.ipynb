{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = ''\n",
    "path_out = root_path + ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_dataset import EvaluationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 3, 5, 10, 15, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_path = root_path + 'results/evaluation_data_deepconn_fm.csv'\n",
    "collab_df = pd.read_csv(collab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_path = root_path + 'results/evaluation_data_node2vec_update.csv'\n",
    "bow_df = pd.read_csv(bow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_list(list_string):\n",
    "    return ast.literal_eval(list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(ranking, k):\n",
    "    return 1.0 / (k + ranking)\n",
    "    \n",
    "def rrf_score(ranking_collab, ranking_bow, k=60):\n",
    "    r1 = r(ranking_collab, k)\n",
    "    r2 = r(ranking_bow, k)\n",
    "    return sum([r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = EvaluationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_out, mode='w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['author_id', 'k', 'hits_at_k', 'interacted_count', 'precision', 'recall', 'AP', 'documents', 'ranking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "evaluation_dataset.current_line = 0\n",
    "for author_id, user_rep, article_ids, le, y_t in tqdm(evaluation_dataset, total=len(evaluation_dataset)):\n",
    "    ranking_collab = parse_input_list(collab_df.iloc[pos]['ranking'])\n",
    "    ranking_bow = parse_input_list(bow_df.iloc[pos]['ranking'])\n",
    "    \n",
    "    ranking_collab = [int(x) for x in ranking_collab]\n",
    "    ranking_bow = [int(x) for x in ranking_bow]\n",
    "    scores = []\n",
    "    y = []\n",
    "    for article_id in article_ids:\n",
    "        if article_id == article_ids[0]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        interacted_pos_collab = ranking_collab.index(article_id)\n",
    "        interacted_pos_bow = ranking_bow.index(article_id)\n",
    "        new_score = rrf_score(interacted_pos_collab, interacted_pos_bow)\n",
    "        scores.append(new_score)\n",
    "        \n",
    "    out = pd.DataFrame({'v': scores, 'flag': y, 'article_ids': article_ids})\n",
    "    out = out.sort_values(by='v', ascending=False)\n",
    "    article_ids_ranked = out['article_ids'].tolist()\n",
    "    y_true = out['flag'].tolist()\n",
    "    \n",
    "    interacted_items_count_testset = 1\n",
    "    \n",
    "    for k in K:\n",
    "        hits_at_k = sum(y_true[:k])\n",
    "        interacted_items_count_testset, precision, recall = evaluate(interacted_items_count_testset, hits_at_k, k)\n",
    "        precisions_at = []\n",
    "        for i, el in enumerate(y_true[:k]):\n",
    "            precisions_at.append(sum(y_true[:i + 1]) / (i + 1))\n",
    "\n",
    "        AP_at_k = sum(precisions_at) / k\n",
    "\n",
    "        with open(path_out, 'a', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # ['author_id', 'k', 'hits_at_k', 'interacted_count', 'precision', 'recall']\n",
    "            writer.writerow(\n",
    "                [author_id, k, hits_at_k, interacted_items_count_testset, precision, recall, round(AP_at_k, 4),\n",
    "                 len(out), article_ids_ranked])\n",
    "\n",
    "    pos += len(K)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(root_path + 'results/evaluation_fusion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos += len(K) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset.test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset.test_df[evaluation_dataset.test_df['article_id'] == 4419688.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
